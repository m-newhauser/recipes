{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnY/vjCwcxXpDvWHhYogTK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c4cb5a4387f460ebdd8e95d0f20dde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a74a24484b5742f7a50510255b3bf82f",
              "IPY_MODEL_8f7d7d8c4d174215adbc9290179f819d",
              "IPY_MODEL_6b2cdfd163304cebbf2d339b87e03320"
            ],
            "layout": "IPY_MODEL_497c72243e22484c9b3df56fad898bc5"
          }
        },
        "a74a24484b5742f7a50510255b3bf82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9551c9540d408d96fdda2fc1036774",
            "placeholder": "​",
            "style": "IPY_MODEL_5767e0485000455d9ee90e47d9d86acd",
            "value": "Fetching 9 files: 100%"
          }
        },
        "8f7d7d8c4d174215adbc9290179f819d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2795a6f0df034ab5b7c24a00b549d525",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_032efd656bb94514b3731e91fe031d70",
            "value": 9
          }
        },
        "6b2cdfd163304cebbf2d339b87e03320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f688f030aed45bbb8223d54032592e2",
            "placeholder": "​",
            "style": "IPY_MODEL_1d52c5829b4c4c17a5965dcaa1ca37db",
            "value": " 9/9 [00:00&lt;00:00, 390.55it/s]"
          }
        },
        "497c72243e22484c9b3df56fad898bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9551c9540d408d96fdda2fc1036774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5767e0485000455d9ee90e47d9d86acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2795a6f0df034ab5b7c24a00b549d525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "032efd656bb94514b3731e91fe031d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f688f030aed45bbb8223d54032592e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d52c5829b4c4c17a5965dcaa1ca37db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-newhauser/recipes/blob/main/docling_weaviate_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing RAG over PDFs with Weaviate and Docling\n",
        "## A recipe 🧑‍🍳 🐥 💚\n",
        "By Mary Newhauser, MLE @ Weaviate\n",
        "\n",
        "This is a code recipe that uses [Weaviate](https://weaviate.io/) to perform RAG over PDF documents parsed by [Docling](https://ds4sd.github.io/docling/).\n",
        "\n",
        "In this notebook, we accomplish the following:\n",
        "* Parse the top machine learning papers on [arXiv](https://arxiv.org/) using Docling\n",
        "* Perform hierarchical chunking of the documents using Docling\n",
        "* Generate text embeddings with OpenAI\n",
        "* Perform RAG using [Weaviate](https://weaviate.io/developers/weaviate/search/generative)\n",
        "\n",
        "To run this notebook, you'll need:\n",
        "* An [OpenAI API key](https://platform.openai.com/docs/quickstart)\n",
        "* Access to GPU/s"
      ],
      "metadata": {
        "id": "Ag9kcX2B_atc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Docling and Weaviate client\n",
        "\n",
        "Note: If Colab prompts you to restart the session after running the cell below, click \"restart\" and proceed with running the rest of the notebook."
      ],
      "metadata": {
        "id": "4YgT7tpXCUl0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u076oUSF_YUG"
      },
      "outputs": [],
      "source": [
        "%pip install docling==\"2.7.0\"\n",
        "%pip install -U weaviate-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🐥 Part 1: Docling"
      ],
      "metadata": {
        "id": "2q2F9RUmR8Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check that GPU is enabled\n",
        "assert 'GPU' in [device.device_type for device in tf.config.list_physical_devices()], \"GPU is not enabled. Please go to Runtime > Change runtime type > Hardware accelerator and select GPU.\""
      ],
      "metadata": {
        "id": "5xYNA9vrC08S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we've collected 10 influential machine learning papers published as PDFs on arXiv. Because Docling does not yet have title extraction for PDFs, we manually add the titles in a corresponding list.\n",
        "\n",
        "Note: Converting all 10 papers should take around 8 minutes with a T4 GPU."
      ],
      "metadata": {
        "id": "wHTsy4a8JFPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Influential machine learning papers\n",
        "source_urls = [\n",
        "    \"https://arxiv.org/pdf/1706.03762\",\n",
        "    \"https://arxiv.org/pdf/1810.04805\",\n",
        "    \"https://arxiv.org/pdf/1406.2661\",\n",
        "    \"https://arxiv.org/pdf/1409.0473\",\n",
        "    \"https://arxiv.org/pdf/1412.6980\",\n",
        "    \"https://arxiv.org/pdf/1312.6114\",\n",
        "    \"https://arxiv.org/pdf/1312.5602\",\n",
        "    \"https://arxiv.org/pdf/1512.03385\",\n",
        "    \"https://arxiv.org/pdf/1409.3215\",\n",
        "    \"https://arxiv.org/pdf/1301.3781\"\n",
        "]\n",
        "\n",
        "# And their corresponding titles (because Docling doesn't have title extraction yet!)\n",
        "source_titles = [\n",
        "    \"Attention Is All You Need\",\n",
        "    \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\n",
        "    \"Generative Adversarial Nets\",\n",
        "    \"Neural Machine Translation by Jointly Learning to Align and Translate\",\n",
        "    \"Adam: A Method for Stochastic Optimization\",\n",
        "    \"Auto-Encoding Variational Bayes\",\n",
        "    \"Playing Atari with Deep Reinforcement Learning\",\n",
        "    \"Deep Residual Learning for Image Recognition\",\n",
        "    \"Sequence to Sequence Learning with Neural Networks\",\n",
        "    \"A Neural Probabilistic Language Model\"\n",
        "]"
      ],
      "metadata": {
        "id": "Vy5SMPiGDMy-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert PDFs to Docling documents\n",
        "\n",
        "Here we use Docling's `.convert_all()` to parse a batch of PDFs. The result is a list of Docling documents that we can use for text extraction."
      ],
      "metadata": {
        "id": "5fi8wzHrCoLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.datamodel.document import ConversionResult\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# Instantiate the doc converter\n",
        "doc_converter = DocumentConverter()\n",
        "\n",
        "# Directly pass list of files or streams to `convert_all`\n",
        "conv_results_iter = doc_converter.convert_all(source_urls) # previously `convert`\n",
        "\n",
        "# Iterate over the generator to get a list of Docling documents\n",
        "docs = [result.document for result in conv_results_iter]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0c4cb5a4387f460ebdd8e95d0f20dde2",
            "a74a24484b5742f7a50510255b3bf82f",
            "8f7d7d8c4d174215adbc9290179f819d",
            "6b2cdfd163304cebbf2d339b87e03320",
            "497c72243e22484c9b3df56fad898bc5",
            "ec9551c9540d408d96fdda2fc1036774",
            "5767e0485000455d9ee90e47d9d86acd",
            "2795a6f0df034ab5b7c24a00b549d525",
            "032efd656bb94514b3731e91fe031d70",
            "8f688f030aed45bbb8223d54032592e2",
            "1d52c5829b4c4c17a5965dcaa1ca37db"
          ]
        },
        "id": "Sr44xGR1PNSc",
        "outputId": "d6b821d8-de08-4705-f90a-01618a2b5918"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c4cb5a4387f460ebdd8e95d0f20dde2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERR#: COULD NOT CONVERT TO RS THIS TABLE TO COMPUTE SPANS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-process extracted document data\n",
        "#### Perform hierarchical chunking on documents\n",
        "\n",
        "We use Docling's `HierarchicalChunker()` to perform hierarchy-aware chunking of our list of documents. This is meant to preserve some of the structure and relationships within the document, which enables more accurate and relevant retrieval in our RAG pipeline."
      ],
      "metadata": {
        "id": "xHun_P-OCtKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docling_core.transforms.chunker import HierarchicalChunker\n",
        "\n",
        "# Initialize lists for text, and titles\n",
        "texts, titles = [], []\n",
        "\n",
        "chunker = HierarchicalChunker()\n",
        "\n",
        "# Process each document in the list\n",
        "for doc, title in zip(docs, source_titles):       # Pair each document with its title\n",
        "    chunks = list(chunker.chunk(doc))             # Perform hierarchical chunking and get text from chunks\n",
        "    for chunk in chunks:\n",
        "        texts.append(chunk.text)\n",
        "        titles.append(title)"
      ],
      "metadata": {
        "id": "L17ju9xibuIo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're splitting the documents into chunks, we'll concatenate the article title to the beginning of each chunk for additional context."
      ],
      "metadata": {
        "id": "khbU9R1li2Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate title and text\n",
        "for i in range(len(texts)):\n",
        "    texts[i] = f\"{titles[i]} {texts[i]}\""
      ],
      "metadata": {
        "id": "HNwYV9P57OwF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💚 Part 2: Weaviate\n",
        "### Create and configure an embedded Weaviate collection"
      ],
      "metadata": {
        "id": "uhLlCpQODaT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Embedded Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) allows you to spin up a Weaviate instance directly from your application code, without having to use a Docker container.\n",
        "\n",
        "If you're interested in other deployment methods, like using Docker-Compose or Kubernetes, check out this [page](https://weaviate.io/developers/weaviate/installation) in the Weaviate docs.\n",
        "\n",
        "We'll be using the OpenAI API for both generating the text embeddings and for the generative model in our RAG pipeline. Make sure to change the code below to ingest your own API key."
      ],
      "metadata": {
        "id": "ho7xYQTZK5Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Fetch your OpenAI API key\n",
        "openai_api_key = userdata.get(\"OPENAI_APIKEY\")\n",
        "\n",
        "# Connect to Weaviate embedded\n",
        "client = weaviate.connect_to_embedded(\n",
        "    headers={\n",
        "        \"X-OpenAI-Api-Key\": openai_api_key\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFUBEZiJUMic",
        "outputId": "f580f189-77f1-4d8d-d4d6-d777a7a3a246"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:weaviate-client:Started /root/.cache/weaviate-embedded: process ID 86732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate.classes.config as wc\n",
        "from weaviate.classes.config import Property, DataType\n",
        "\n",
        "# Define the collection name\n",
        "collection_name = \"docling\"\n",
        "\n",
        "# Delete the collection if it already exists\n",
        "if (client.collections.exists(collection_name)):\n",
        "    client.collections.delete(collection_name)\n",
        "\n",
        "# Create the collection\n",
        "collection = client.collections.create(\n",
        "    name=collection_name,\n",
        "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(\n",
        "        model=\"text-embedding-3-large\",                           # Specify your embedding model here\n",
        "    ),\n",
        "\n",
        "    # Enable generative model from Cohere\n",
        "    generative_config=wc.Configure.Generative.openai(\n",
        "    model=\"gpt-4o\"                                                # Specify your generative model for RAG here\n",
        "    ),\n",
        "\n",
        "    # Define properties of metadata\n",
        "    properties=[\n",
        "        wc.Property(\n",
        "            name=\"text\",\n",
        "            data_type=wc.DataType.TEXT\n",
        "        ),\n",
        "        wc.Property(\n",
        "            name=\"title\",\n",
        "            data_type=wc.DataType.TEXT,\n",
        "            skip_vectorization=True\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "4nu9qM75hrsd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrangle data into an acceptable format for Weaviate\n",
        "\n",
        "Transform our data from lists to a list of dictionaries for insertion into our Weaviate collection."
      ],
      "metadata": {
        "id": "RgMcZDB9Dzfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the data object\n",
        "data = []\n",
        "\n",
        "# Create a dictionary for each row by iterating through the corresponding lists\n",
        "for text, title in zip(texts, titles):\n",
        "    data_point = {\n",
        "        \"text\": text,\n",
        "        \"title\": title,\n",
        "    }\n",
        "    data.append(data_point)"
      ],
      "metadata": {
        "id": "kttDgwZEsIJQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insert data into Weaviate and generate embeddings\n",
        "\n",
        "Embeddings will be generated upon insertion to our Weaviate collection."
      ],
      "metadata": {
        "id": "-4amqRaoD5g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert text chunks and metadata into vector DB collection\n",
        "response = collection.data.insert_many(\n",
        "    data\n",
        ")\n",
        "\n",
        "if (response.has_errors):\n",
        "    print(response.errors)\n",
        "else:\n",
        "    print(\"Insert complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8VCYnhbaxcz",
        "outputId": "a2a37eec-85f4-405b-d6a2-4cccba259e29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query the data\n",
        "\n",
        "Here, we perform a simple similarity search to return the most similar embedded chunks to our search query."
      ],
      "metadata": {
        "id": "KI01PxjuD_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "response = collection.query.near_text(\n",
        "    query=\"bert\",\n",
        "    limit=2,\n",
        "    return_metadata=MetadataQuery(distance=True),\n",
        "    return_properties=[\"text\", \"title\"]\n",
        ")\n",
        "\n",
        "for o in response.objects:\n",
        "    print(o.properties)\n",
        "    print(o.metadata.distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbz6nWJc5CSj",
        "outputId": "1d2dbbc1-0405-4b98-8307-c89fc5ab31e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding A distinctive feature of BERT is its unified architecture across different tasks. There is mini-', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}\n",
            "0.6580140590667725\n",
            "{'text': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding We introduce a new language representation model called BERT , which stands for B idirectional E ncoder R epresentations from T ransformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}\n",
            "0.6695281863212585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform RAG on parsed articles\n",
        "\n",
        "Weaviate's `generate` module allows you to perform RAG over your embedded data without having to use a separate framework.\n",
        "\n",
        "We specify a prompt that includes the field we want to search through in the database (in this case it's `text`), a query that includes our search term, and the number of retrieved results to use in the generation."
      ],
      "metadata": {
        "id": "elo32iMnEC18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "\n",
        "# Create a prompt where context from the Weaviate collection will be injected\n",
        "prompt = \"Explain how {text} works, using only the retrieved context.\"\n",
        "query = \"bert\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query,\n",
        "    limit=3,\n",
        "    grouped_task=prompt,\n",
        "    return_properties=[\"text\", \"title\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "\n",
        "console.print(Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "7r2LMSX9bO4y",
        "outputId": "7c9989a7-960b-46a8-e8a3-8471958004b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31m╭─\u001b[0m\u001b[1;31m───────────────────────────────────────────────────\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31m────────────────────────────────────────────────────\u001b[0m\u001b[1;31m─╮\u001b[0m\n",
              "\u001b[1;31m│\u001b[0m Explain how bert works, using only the retrieved context.                                                       \u001b[1;31m│\u001b[0m\n",
              "\u001b[1;31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">╭──────────────────────────────────────────────────── Prompt ─────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">│</span> Explain how bert works, using only the retrieved context.                                                       <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32m╭─\u001b[0m\u001b[1;32m──────────────────────────────────────────────\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32m──────────────────────────────────────────────\u001b[0m\u001b[1;32m─╮\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation    \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m model designed to pretrain deep bidirectional representations from unlabeled text. It conditions on both left   \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m and right context in all layers, allowing it to capture context more effectively than traditional left-to-right \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m or right-to-left models. BERT's unified architecture can be fine-tuned with just one additional output layer to \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m achieve state-of-the-art performance on various tasks, such as question answering and language inference,       \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m without needing substantial task-specific architecture modifications. The pre-training involves two             \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m unsupervised tasks, which are not specified in the provided context.                                            \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╭─────────────────────────────────────────────── Generated Content ───────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> BERT, which stands for Bidirectional Encoder Representations from Transformers, is a language representation    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> model designed to pretrain deep bidirectional representations from unlabeled text. It conditions on both left   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> and right context in all layers, allowing it to capture context more effectively than traditional left-to-right <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> or right-to-left models. BERT's unified architecture can be fine-tuned with just one additional output layer to <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> achieve state-of-the-art performance on various tasks, such as question answering and language inference,       <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> without needing substantial task-specific architecture modifications. The pre-training involves two             <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> unsupervised tasks, which are not specified in the provided context.                                            <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt where context from the Weaviate collection will be injected\n",
        "prompt = \"Explain how {text} works, using only the retrieved context.\"\n",
        "query = \"a generative adversarial net\"\n",
        "\n",
        "response = collection.generate.near_text(\n",
        "    query=query,\n",
        "    limit=3,\n",
        "    grouped_task=prompt,\n",
        "    return_properties=[\"text\", \"title\"]\n",
        ")\n",
        "\n",
        "# Prettify the output using Rich\n",
        "console = Console()\n",
        "\n",
        "console.print(Panel(f\"{prompt}\".replace(\"{text}\", query), title=\"Prompt\", border_style=\"bold red\"))\n",
        "console.print(Panel(response.generated, title=\"Generated Content\", border_style=\"bold green\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "Dtju3oCiDOdD",
        "outputId": "a08ee7ea-c195-4c35-9230-97e4e6e224d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31m╭─\u001b[0m\u001b[1;31m───────────────────────────────────────────────────\u001b[0m\u001b[1;31m Prompt \u001b[0m\u001b[1;31m────────────────────────────────────────────────────\u001b[0m\u001b[1;31m─╮\u001b[0m\n",
              "\u001b[1;31m│\u001b[0m Explain how a generative adversarial net works, using only the retrieved context.                               \u001b[1;31m│\u001b[0m\n",
              "\u001b[1;31m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">╭──────────────────────────────────────────────────── Prompt ─────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">│</span> Explain how a generative adversarial net works, using only the retrieved context.                               <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32m╭─\u001b[0m\u001b[1;32m──────────────────────────────────────────────\u001b[0m\u001b[1;32m Generated Content \u001b[0m\u001b[1;32m──────────────────────────────────────────────\u001b[0m\u001b[1;32m─╮\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m Generative Adversarial Nets (GANs) operate within an adversarial framework where two models are trained         \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m simultaneously: a generative model (G) and a discriminative model (D). The generative model aims to capture the \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m data distribution and produce samples that mimic real data, while the discriminative model's task is to         \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m distinguish between samples from the real data and those generated by G. This setup is akin to a game where the \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m generative model acts like counterfeiters trying to create indistinguishable fake currency, and the             \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m discriminative model acts like the police trying to detect these counterfeits.                                  \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m                                                                                                                 \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m The training process involves a minimax two-player game where G tries to maximize the probability of D making a \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m mistake, while D tries to accurately classify the samples. When both models are defined by multilayer           \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m perceptrons, they can be trained using backpropagation without the need for Markov chains or approximate        \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m inference networks. The ultimate goal is for G to perfectly replicate the training data distribution, making    \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m D's output equal to 1/2 everywhere, indicating it cannot distinguish between real and generated samples. This   \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m framework allows for specific training algorithms and can generate samples by passing random noise through the  \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m│\u001b[0m generative model using forward propagation.                                                                     \u001b[1;32m│\u001b[0m\n",
              "\u001b[1;32m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╭─────────────────────────────────────────────── Generated Content ───────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> Generative Adversarial Nets (GANs) operate within an adversarial framework where two models are trained         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> simultaneously: a generative model (G) and a discriminative model (D). The generative model aims to capture the <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> data distribution and produce samples that mimic real data, while the discriminative model's task is to         <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> distinguish between samples from the real data and those generated by G. This setup is akin to a game where the <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> generative model acts like counterfeiters trying to create indistinguishable fake currency, and the             <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> discriminative model acts like the police trying to detect these counterfeits.                                  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> The training process involves a minimax two-player game where G tries to maximize the probability of D making a <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> mistake, while D tries to accurately classify the samples. When both models are defined by multilayer           <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> perceptrons, they can be trained using backpropagation without the need for Markov chains or approximate        <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> inference networks. The ultimate goal is for G to perfectly replicate the training data distribution, making    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> D's output equal to 1/2 everywhere, indicating it cannot distinguish between real and generated samples. This   <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> framework allows for specific training algorithms and can generate samples by passing random noise through the  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span> generative model using forward propagation.                                                                     <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">│</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our RAG pipeline performs relatively well for simple queries, especially given the small size of the dataset. Scaling this method for converting a larger sample of PDFs would require more compute (GPUs) and a more advanced deployment of Weaviate (like Docker, Kubernetes, or Weaviate Cloud). However, this notebook demonstrates that Docling is a robust and powerful open source tool for converting PDFs to structured data.\n",
        "\n",
        "To take this solution to next level, you consider:\n",
        "* Experimenting with different [chunking techniques](https://weaviate.io/developers/academy/py/standalone/chunking/introduction)\n",
        "* Using a RAG framework like [DSPy](https://weaviate.io/developers/integrations/llm-frameworks/dspy), [LlamaIndex](https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/weaviate/), or [LangChain](https://python.langchain.com/docs/integrations/vectorstores/weaviate/)\n",
        "* Implementing [advanced RAG](https://weaviate.io/blog/advanced-rag) techniques, like [Agentic RAG](https://weaviate.io/blog/what-is-agentic-rag)"
      ],
      "metadata": {
        "id": "7tGz49nfUegG"
      }
    }
  ]
}